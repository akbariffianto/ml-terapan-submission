# -*- coding: utf-8 -*-
"""Anime Recomender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lATqoByF64jBBQWC6MATyVQ9Gdm7brxI
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

"""#Load Data

Langkah pertama yang dilakukan adalah meload dataset yang akan digunakan, dalam projek kali ini akan menggunakan dataset dengan tema anime atau kartun dari negara jepang
"""

anime = pd.read_csv('/content/anime.csv')
rating = pd.read_csv('/content/rating.csv')

"""Masing masing dataset diberi nama sesuai dengan nama dataset untuk mempermudah langkah selanjutnya

Dataset dengan nama anime memiliki kolom anime_id, name, genre, type, episodes, rating, dan members
"""

rating.head()

"""Dataset dengan nama rating memiliki kolom user_id, anime_id, dan rating"""

print('Jumlah data anime: ', len(anime.name.unique()))
print('Jumlah data rating: ', len(rating.user_id.unique()))

"""Dapat disimpulkan dataset memiliki jumlah judul anime sebanyak 12292 dan jumlah user sebanyak 73515

#EDA

##Anime

###Deskripsi Variabel
"""

anime.head()

"""Variabel pada data anime:
- anime_id : Index setiap anime
- name : Judul anime
- genre : Genre anime
- type : Jenis penyiaran
- episodes : Jumlah episode
- rating : Rating anime
- Members : Jumlah member yang bergabung ke dalam komunitas
"""

anime.info()

"""Berdasarkan output diatas dapat disimpulkan dataset anime memiliki entri sebanyak 12294. Namun terdapat null value di beberapa variabel. Tindakan cleaning data perlu dilakukan.

###Univariate Analysis
"""

print('Jumlah anime: ', len(anime.name.unique()))

plt.figure(figsize=(8, 6))
sns.histplot(anime['rating'].dropna(), kde=True)
plt.title('Distribution of Anime Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

max_rating = rating['rating'].max()
mean_rating = rating['rating'].mean()
min_rating = rating['rating'].min()

print(f"Max Rating: {max_rating}")
print(f"Mean Rating: {mean_rating}")
print(f"Min Rating: {min_rating}")

plt.figure(figsize=(8, 6))
sns.histplot(anime['episodes'].dropna(), kde=True, orientation='vertical')
plt.title('Distribution of Anime Episodes')
plt.ylabel('Frecuency')
plt.xlabel('Episodes')
plt.show()

print('Jumlah episode anime: ', len(anime.episodes.unique()))

top_10_episodes = anime['episodes'].value_counts().nlargest(10)
top_10_episodes

anime['members'] = anime['members'].fillna(0)

plt.figure(figsize=(10, 6))
sns.scatterplot(x='episodes', y='members', data=anime)
plt.title('Episodes vs. Members')
plt.xlabel('Number of Episodes')
plt.ylabel('Number of Members')
plt.show()

high_member_anime = anime[anime['members'] > 100000]
print(high_member_anime[['name', 'episodes', 'members']])

plt.figure(figsize=(10, 6))
sns.countplot(x='type', data=anime)
plt.title('Distribution of Anime Types')
plt.xlabel('Anime Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

genre_counts = anime['genre'].str.split(',').explode().value_counts().head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=genre_counts.index, y=genre_counts.values)
plt.title('Top 10 Anime Genres')
plt.xlabel('Genre')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

unique_genres = []
for index in anime.index:
    genres = anime.loc[index, 'genre']
    if isinstance(genres, str):
      for genre in genres.split(','):
        genre = genre.strip()
        if genre not in unique_genres:
          unique_genres.append(genre)

print("Jumlah unik genre:", len(unique_genres))

top_10_members = anime.sort_values(by='members', ascending=False).head(10)
top_10_members

plt.figure(figsize=(12, 6))
sns.histplot(anime['members'], kde=True)
plt.title('Distribution of Anime Members')
plt.xlabel('Number of Members')
plt.ylabel('Frequency')
plt.show()

"""###Multivariate Analysis"""

members_rating_corr = anime['members'].corr(anime['rating'], method='spearman')
plt.figure(figsize=(6, 4))
sns.heatmap(anime[['members', 'rating']].corr(method='spearman'), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Spearman Correlation Heatmap: Members vs. Rating')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='type', y='rating', data=anime)
plt.title('Rating Distribution by Anime Type')
plt.xlabel('Anime Type')
plt.ylabel('Rating')
plt.xticks(rotation=45)
plt.show()

"""###Data Preparation

####Cleaning Data
"""

anime.isna().sum()

anime.dropna(subset=['genre'], inplace=True)

anime.isna().sum()

anime.drop_duplicates('name')

anime.shape

"""Dari proses membersihkan data yang duplikat dan memiliki value null dataset anime memiliki entri sebanyak 12294

####Feature Engineering

Dataset anime akan digunakan dalam model content based filtering berdasarkan genre tiap judul anime. Oleh karena itu, dalam pengembangan modelnya hanya membutuhkan anime_id, name, dan genre.
"""

cbf_anime = anime[['anime_id', 'name', 'genre']]
cbf_anime.head()

cbf_anime.loc[cbf_anime['genre'].str.contains('Sci-Fi', na=False), 'genre'] = cbf_anime['genre'].str.replace('Sci-Fi', 'scifi')
cbf_anime.loc[cbf_anime['genre'].str.contains('Slice of Life', na=False), 'genre'] = cbf_anime['genre'].str.replace('Slice of Life', 'sliceoflife')

cbf_anime.head()

# Mengonversi data series ‘anime_id’ menjadi dalam bentuk list
anime_id = cbf_anime['anime_id'].tolist()

# Mengonversi data series 'name' menjadi dalam bentuk list
anime_name = cbf_anime['name'].tolist()

# Mengonversi data series 'genre' menjadi dalam bentuk list
anime_genre = cbf_anime['genre'].tolist()

"""##Rating

###Deksripsi Variabel
"""

rating.head()

"""Dari output diatas, variabel rating:
- user_id : indeks user yang memberikan rating
- anime_id : indeks anime yang dirating
- rating : skor penilaian yang diberikan
"""

rating.shape

"""Berdasarkan output, diketahui data rating memiliki 7813737  entri.

###Univariate Analysis
"""

plt.figure(figsize=(8, 6))
sns.countplot(x='rating', data=rating)
plt.title('Count of Each Rating')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

"""###Cleaning Data"""

rating.isna().sum()

"""###Feature Engineering

Dataset rating telah memiliki fitur data yang diperlukan untuk mengembangkan model collaborative filtering nantinya.

###Data Preparation

Dalam melakukan collaborative filtering, penting untuk membatasi jumlah pengguna yang memberikan rating pada setiap item karena beberapa alasan utama yang berkaitan dengan kualitas rekomendasi dan efisiensi sistem.

1. Masalah Sparsity
Membatasi jumlah pengguna membantu mengurangi sparsity pada matriks user-item, sehingga mempermudah sistem menemukan pola dan menghasilkan rekomendasi yang lebih akurat.

2. Cold Start Problem
Dengan membatasi jumlah pengguna, sistem dapat lebih fokus pada data yang ada, mengurangi dampak dari item atau pengguna baru yang belum memiliki cukup informasi.

3. Efisiensi Komputasi
Membatasi data membuat proses perhitungan similarity dan prediksi lebih cepat dan efisien, sehingga meningkatkan pengalaman pengguna.

Langkah pertama untuk dapat membatasi pemberian penilaian nantinya harus memeriksa tiap entri user yang memberikan rating terlebih dahulu
"""

print('Jumlah user_id: ', len(rating.user_id.unique()))
print('Jumlah anime_id: ', len(rating.anime_id.unique()))

"""Bisa disimpulkan ada fakta dimana user memberikan rating lebih dari satu anime"""

rating_per_user = rating.groupby('user_id').size()

# Melihat jumlah rata rata anime yang dirating oleh user
print('Jumlah rata_rata anime yang dirating oleh user: ', rating_per_user.mean())

"""Dari informasi tersebut, maka jumlah batasan pemberian rating adalah 50 untuk menjaga informasi tetap banyak sehingga akurasi bisa tinggi di pengembangan model. User yang tidak memberikan rating sebanyak 50 maka akan dihapus sedangkan yang memberikan rating lebih dari 50, pemberian ratingnya akan dihapus."""

# Mendapatkan user yang memberikan setidaknya 50 rating
valid_users = rating_per_user[rating_per_user >= 50].index

# Menyaring dataset untuk hanya menyertakan user yang valid
cf_rating = rating[rating['user_id'].isin(valid_users)]

# Membatasi jumlah rating per user hingga maksimal 50
cf_rating = (
    cf_rating.groupby('user_id')
    .head(50)  # Mengambil 50 rating pertama untuk setiap user
    .reset_index(drop=True)
)

print('Dataset setelah filter:')
print(cf_rating.head())

cf_rating.shape

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = cf_rating['user_id'].unique().tolist()
print('list user_id: ', user_ids)

# Melakukan encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user_id : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke user_id: ', user_encoded_to_user)

# Mengubah anime_id menjadi list tanpa nilai yang sama
anime_ids = cf_rating['anime_id'].unique().tolist()
print('list anime_id: ', anime_ids)

# Melakukan proses encoding anime_id
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}
print('encoded anime_id : ', anime_to_anime_encoded)

# Melakukan proses encoding angka ke anime_id
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}
print('encoded angka ke anime_id: ', anime_encoded_to_anime)

# Mapping userID ke dataframe user
cf_rating['user'] = cf_rating['user_id'].map(user_to_user_encoded)

# Mapping placeID ke dataframe anime
cf_rating['anime'] = cf_rating['anime_id'].map(anime_to_anime_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah anime
num_anime = len(anime_encoded_to_anime)
print(num_anime)

# Mengubah rating menjadi nilai float
cf_rating['rating'] = cf_rating['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(cf_rating['rating'])

# Nilai maksimal rating
max_rating = max(cf_rating['rating'])

print('Number of User: {}, Number of Anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""#Content Based Filtering"""

# Membuat dictionary untuk data , 'anime_id', 'name', 'genre'
fix_anime = pd.DataFrame({
    'id': anime_id,
    'name': anime_name,
    'genre': anime_genre
})
fix_anime.head()

"""##One-Hot Encoding"""

genre_list = []

# Membuat daftar genre unik
for index in fix_anime.index:
    temp = fix_anime['genre'][index].split(',')
    for i in temp:
        if i not in genre_list:
            genre_list.append(i)

onehot_df = pd.DataFrame(0, index=fix_anime.index, columns=genre_list)

# Mengisi nilai 1 untuk genre yang sesuai
for index in fix_anime.index:
    temp = fix_anime['genre'][index].split(',')
    for i in temp:
        onehot_df.loc[index, i] = 1

fix_anime = pd.concat([fix_anime, onehot_df], axis=1).fillna(0)
print(fix_anime.head())

"""##TF-IDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data genre
tf.fit(fix_anime['genre'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(fix_anime['genre'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan jenis masakan
# Baris diisi dengan nama anime

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=fix_anime.name
).sample(22, axis=1).sample(10, axis=0)

"""##Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama anime
cosine_sim_df = pd.DataFrame(cosine_sim, index=fix_anime['name'], columns=fix_anime['name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap anime
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""##Euclidean Distance"""

from sklearn.metrics.pairwise import euclidean_distances

euclidean_sim = euclidean_distances(tfidf_matrix)
euclidean_sim

euclidean_sim_df = pd.DataFrame(euclidean_sim, index=fix_anime['name'], columns=fix_anime['name'])
print('Shape:', euclidean_sim_df.shape)

euclidean_sim_df.sample(5, axis=1).sample(10, axis=0)

"""##Mendapatkan Rekomendasi"""

cbf_anime[cbf_anime.name.eq('Naruto')]

def anime_cosine(nama_anime, similarity_data=cosine_sim_df, items=fix_anime[['name', 'genre']], k=10):
    """
    Rekomendasi Anime berdasarkan kemiripan Genre

    Parameter:
    ---
    nama_anime : tipe data string (str)
                Nama anime (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan anime sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_anime].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop name agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_anime, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

# Mendapatkan rekomendasi anime yang mirip
anime_cosine('Naruto')

def anime_euclidean(nama_anime, similarity_data=euclidean_sim_df, items=fix_anime[['name', 'genre']], k=10):
    """
    Rekomendasi Anime berdasarkan jarak Euclidean

    Parameter:
    ---
    nama_anime : tipe data string (str)
                Nama anime (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan anime sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---
    Pada index ini, kita mengambil k dengan nilai Euclidean distance terkecil
    pada index matrix yang diberikan (i).
    """

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    index = similarity_data.loc[:, nama_anime].to_numpy().argpartition(range(k))

    # Mengambil data dengan Euclidean distance terkecil dari index yang ada
    closest = similarity_data.columns[index[:k]]

    # Drop name agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_anime, errors='ignore')

    # Membuat DataFrame hasil
    result_euclidean = pd.DataFrame(closest).merge(items).head(k)

    # Return DataFrame hasil
    return result_euclidean

anime_euclidean('Naruto')

"""#Collaborative Filtering

##Splitting Data
"""

# Mengacak dataset
cf_rating = cf_rating.sample(frac=1, random_state=42)
cf_rating

# Membuat variabel x untuk mencocokkan data user dan anime menjadi satu value
x = cf_rating[['user', 'anime']].values

# Membuat variabel y untuk membuat rating dari hasil
y = cf_rating['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * cf_rating.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

class RecommenderNet(tf.keras.Model):
  # Insialisasi fungsi
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.anime_embedding = layers.Embedding( # layer embeddings anime
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1) # layer embedding anime bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    anime_vector = self.anime_embedding(inputs[:, 1]) # memanggil layer embedding 3
    anime_bias = self.anime_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)

    x = dot_user_anime + user_bias + anime_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_anime, 20) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 128,
    epochs = 15,
    validation_data = (x_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# prompt: gunakan model untuk merekomendasi

def recommend_anime_cf(user_id, model, k=10):
    """
    Rekomendasi anime menggunakan Collaborative Filtering.

    Args:
        user_id: ID pengguna yang ingin direkomendasikan anime.
        model: Model Collaborative Filtering yang sudah dilatih.
        k: Jumlah anime yang direkomendasikan.

    Returns:
        DataFrame dengan anime yang direkomendasikan.
    """

    try:
        user_encoded = user_to_user_encoded[user_id]  # Encode user ID
    except KeyError:
        print(f"User ID {user_id} not found in the dataset.")
        return pd.DataFrame()


    anime_not_watched = cf_rating[~cf_rating['user_id'].isin([user_id])]['anime_id'].unique()
    anime_not_watched = list(set(anime_not_watched).intersection(set(anime_ids)))

    anime_not_watched_encoded = [anime_to_anime_encoded[x] for x in anime_not_watched]

    user_anime_array = np.array([[user_encoded, anime] for anime in anime_not_watched_encoded])

    ratings = model.predict(user_anime_array).flatten()

    top_ratings_indices = ratings.argsort()[-k:][::-1]
    recommended_anime_ids = [anime_encoded_to_anime[anime_not_watched_encoded[x]] for x in top_ratings_indices]

    recommended_anime = cbf_anime[cbf_anime['anime_id'].isin(recommended_anime_ids)][['name', 'genre']].reset_index(drop=True)
    return recommended_anime

recommend_anime_cf(1,model)

# prompt: evaluasi model yang dibuat

def evaluate_cf_recommendations(recommendations, user_id, rating_df):
    """
    Evaluates collaborative filtering recommendations by comparing recommended anime genres to the user's past ratings.

    Args:
        recommendations: DataFrame of recommended anime with 'name' and 'genre' columns.
        user_id: The ID of the user for whom recommendations were generated.
        rating_df: The DataFrame containing user ratings.

    Returns:
        A dictionary containing evaluation metrics.
    """

    user_rated_anime = rating_df[rating_df['user_id'] == user_id]
    user_genres = []
    for _, row in user_rated_anime.iterrows():
      anime_id = row['anime_id']
      try:
        user_genres.extend(cbf_anime[cbf_anime['anime_id'] == anime_id]['genre'].iloc[0].split(','))
      except IndexError:
        # Handle cases where anime_id is not found in cbf_anime
        print(f"Anime ID {anime_id} not found in cbf_anime dataframe")
        continue
    user_genres = list(set(user_genres))


    matching_genres_count = 0
    for _, row in recommendations.iterrows():
        recommended_genres = row['genre'].split(',')
        for genre in recommended_genres:
            if genre in user_genres:
                matching_genres_count += 1
                break

    precision = matching_genres_count / len(recommendations) if len(recommendations) > 0 else 0
    recall = matching_genres_count / len(user_genres) if len(user_genres) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return {
      "precision": precision,
      "recall": recall,
      "f1_score": f1_score,
      "matching_genres_count": matching_genres_count
    }


# Example usage
user_id_to_evaluate = 3  # Replace with the desired user ID
recommendations = recommend_anime_cf(user_id_to_evaluate, model)

if not recommendations.empty:
    evaluation_results = evaluate_cf_recommendations(recommendations, user_id_to_evaluate, cf_rating)

    print(f"Evaluation Results for User {user_id_to_evaluate}:")
    print(f"Precision: {evaluation_results['precision']:.4f}")
    print(f"Recall: {evaluation_results['recall']:.4f}")
    print(f"F1-score: {evaluation_results['f1_score']:.4f}")
    print(f"Matching Genres Count: {evaluation_results['matching_genres_count']}")